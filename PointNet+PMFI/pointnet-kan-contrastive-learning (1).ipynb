{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport urllib.request\nimport zipfile\n\n# URL of the dataset\nurl = \"http://modelnet.cs.princeton.edu/ModelNet40.zip\"\n\n# Directory where the dataset will be downloaded\ndata_dir = \"/kaggle/working/ModelNet40.zip\"  # Use a path within Kaggle's writable area\n\n# Download the dataset\nprint(\"Downloading dataset...\")\nurllib.request.urlretrieve(url, data_dir)\nprint(\"Download completed.\")\n\n# Extract the dataset\nprint(\"Extracting dataset...\")\nwith zipfile.ZipFile(data_dir, 'r') as zip_ref:\n    zip_ref.extractall(os.path.dirname(data_dir))\nprint(\"Extraction completed.\")\n\n# Path to the extracted dataset\nDATA_DIR = os.path.join(os.path.dirname(data_dir), \"ModelNet40\")\nprint(f\"Dataset extracted to {DATA_DIR}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:25:10.742024Z","iopub.execute_input":"2025-04-14T16:25:10.742449Z","iopub.status.idle":"2025-04-14T16:26:28.584640Z","shell.execute_reply.started":"2025-04-14T16:25:10.742415Z","shell.execute_reply":"2025-04-14T16:26:28.583729Z"}},"outputs":[{"name":"stdout","text":"Downloading dataset...\nDownload completed.\nExtracting dataset...\nExtraction completed.\nDataset extracted to /kaggle/working/ModelNet40\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Libraries\n!pip install trimesh\nimport torch\nimport torch.utils.data as data\nimport glob\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport trimesh\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\n\n# Parameter setup\nNUM_POINTS = 1024\nNUM_CLASSES = 40  # ModelNet40\nBATCH_SIZE = 64\npoly_degree = 4  # Polynomial degree of Jacobi Polynomial\nFEATURE = 6  # (x,y,z) + (nx,ny,nz)\nALPHA = 1.0  # \\alpha in Jacobi Polynomial\nBETA = 1.0  # \\beta in Jacobi Polynomial\nSCALE = 3.0  # To control the size of tensor A in the manuscript\nMAX_EPOCHS = 300\nSSL_EPOCHS = 300  # Epochs for self-supervised pre-training\ndirection = '/kaggle/working/ModelNet40'\n\n# Data Augmentation Functions\ndef random_rotation(point_cloud):\n    \"\"\"Apply random rotation to the point cloud.\"\"\"\n    theta = np.random.uniform(0, 2 * np.pi)\n    rotation_matrix = np.array([\n        [np.cos(theta), -np.sin(theta), 0],\n        [np.sin(theta), np.cos(theta), 0],\n        [0, 0, 1]\n    ])\n    rotated_points = np.dot(point_cloud[:, :3], rotation_matrix)\n    return np.concatenate([rotated_points, point_cloud[:, 3:]], axis=1)\n\ndef add_noise(point_cloud, sigma=0.01):\n    \"\"\"Add Gaussian noise to the point cloud.\"\"\"\n    noise = np.random.normal(0, sigma, point_cloud.shape)\n    return point_cloud + noise\n\ndef jitter_point_cloud(point_cloud, sigma=0.01, clip=0.05):\n    \"\"\"Add jitter to the point cloud.\"\"\"\n    noise = np.clip(sigma * np.random.randn(*point_cloud.shape), -clip, clip)\n    return point_cloud + noise\n\ndef augment_point_cloud(point_cloud):\n    \"\"\"Apply a series of augmentations to the point cloud.\"\"\"\n    point_cloud = random_rotation(point_cloud)\n    point_cloud = add_noise(point_cloud)\n    point_cloud = jitter_point_cloud(point_cloud)\n    return point_cloud\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:26:28.585612Z","iopub.execute_input":"2025-04-14T16:26:28.585824Z","iopub.status.idle":"2025-04-14T16:26:42.145736Z","shell.execute_reply.started":"2025-04-14T16:26:28.585805Z","shell.execute_reply":"2025-04-14T16:26:42.144804Z"}},"outputs":[{"name":"stdout","text":"Collecting trimesh\n  Downloading trimesh-4.6.6-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->trimesh) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->trimesh) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->trimesh) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->trimesh) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->trimesh) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20->trimesh) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20->trimesh) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20->trimesh) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->trimesh) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20->trimesh) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20->trimesh) (2024.2.0)\nDownloading trimesh-4.6.6-py3-none-any.whl (709 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.3/709.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trimesh\nSuccessfully installed trimesh-4.6.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class ContrastiveLoss(nn.Module):\n    def __init__(self, temperature=0.5):\n        super(ContrastiveLoss, self).__init__()\n        self.temperature = temperature\n\n    def forward(self, z1, z2):\n        \"\"\"Compute contrastive loss for positive pairs (z1, z2).\"\"\"\n        batch_size = z1.size(0)\n\n        # Normalize embeddings\n        z1 = F.normalize(z1, dim=1)\n        z2 = F.normalize(z2, dim=1)\n\n        # Compute similarity matrix\n        similarity_matrix = torch.exp(torch.mm(z1, z2.T) / self.temperature)\n\n        # Mask to remove self-similarity\n        mask = torch.eye(batch_size, device=z1.device, dtype=torch.bool)\n        positives = similarity_matrix[mask].view(batch_size, -1)\n\n        # Compute negatives\n        negatives = similarity_matrix[~mask].view(batch_size, -1)\n\n        # Compute contrastive loss\n        loss = -torch.log(positives / (positives + torch.sum(negatives, dim=1, keepdim=True)))\n        return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:26:42.147651Z","iopub.execute_input":"2025-04-14T16:26:42.148083Z","iopub.status.idle":"2025-04-14T16:26:42.153834Z","shell.execute_reply.started":"2025-04-14T16:26:42.148061Z","shell.execute_reply":"2025-04-14T16:26:42.153029Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# PointNet-KAN with SSL Support\n# PointNet-KAN with SSL Support\nclass PointNetKANSSL(nn.Module):\n    def __init__(self, input_channels, output_channels, scaling=SCALE):\n        super(PointNetKANSSL, self).__init__()\n        self.jacobikan5 = KANshared(input_channels, int(1024 * scaling), poly_degree)\n        self.jacobikan6 = KAN(int(1024 * scaling), output_channels, poly_degree)\n        self.bn5 = nn.BatchNorm1d(int(1024 * scaling))\n\n        # Projection head for SSL\n        self.projection_head = nn.Sequential(\n            nn.Linear(output_channels, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128)\n        )\n\n    def forward(self, x, return_projection=False):\n        x = self.jacobikan5(x)\n        x = self.bn5(x)\n        global_feature = F.adaptive_max_pool1d(x, output_size=1).squeeze(-1)\n        x = self.jacobikan6(global_feature)\n\n        if return_projection:\n            return self.projection_head(x)\n        return x\n\n# Dataset with SSL Support\nclass PointCloudDataset(Dataset):\n    def __init__(self, points, labels, ssl=False):\n        self.points = points\n        self.labels = labels\n        self.ssl = ssl  # Flag for self-supervised learning\n        self.normalize()\n\n    def normalize(self):\n        for i in range(self.points.shape[0]):\n            spatial_coords = self.points[i, :, :3]\n            normals = self.points[i, :, 3:]\n            centroid = spatial_coords.mean(axis=0, keepdims=True)\n            spatial_coords -= centroid\n            furthest_distance = torch.max(torch.sqrt(torch.sum(spatial_coords ** 2, axis=1, keepdims=True)))\n            spatial_coords /= furthest_distance\n            self.points[i] = torch.cat((spatial_coords, normals), dim=1)\n\n    def __len__(self):\n        return len(self.points)\n\n    def __getitem__(self, idx):\n        point = self.points[idx]\n        label = self.labels[idx]\n\n        if self.ssl:\n            # Return two augmented views for SSL\n            point1 = augment_point_cloud(point.cpu().numpy())\n            point2 = augment_point_cloud(point.cpu().numpy())\n            return torch.tensor(point1, dtype=torch.float32), torch.tensor(point2, dtype=torch.float32), label\n        else:\n            return point, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:26:42.155050Z","iopub.execute_input":"2025-04-14T16:26:42.155370Z","iopub.status.idle":"2025-04-14T16:26:44.887060Z","shell.execute_reply.started":"2025-04-14T16:26:42.155340Z","shell.execute_reply":"2025-04-14T16:26:44.886168Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# KANshared and KAN Classes\nclass KANshared(nn.Module):\n    def __init__(self, input_dim, output_dim, degree, a=ALPHA, b=BETA):\n        super(KANshared, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.a = a\n        self.b = b\n        self.degree = degree\n\n        self.jacobi_coeffs = nn.Parameter(torch.empty(input_dim, output_dim, degree + 1))\n        nn.init.normal_(self.jacobi_coeffs, mean=0.0, std=1 / (input_dim * (degree + 1)))\n\n    def forward(self, x):\n        batch_size, input_dim, num_points = x.shape\n        x = x.permute(0, 2, 1).contiguous() \n        x = torch.tanh(x) \n\n        jacobi = torch.ones(batch_size, num_points, self.input_dim, self.degree + 1, device=x.device)\n\n        if self.degree > 0:\n            jacobi[:, :, :, 1] = ((self.a - self.b) + (self.a + self.b + 2) * x) / 2\n\n        for i in range(2, self.degree + 1):\n            A = (2*i + self.a + self.b - 1)*(2*i + self.a + self.b)/((2*i) * (i + self.a + self.b))\n            B = (2*i + self.a + self.b - 1)*(self.a**2 - self.b**2)/((2*i)*(i + self.a + self.b)*(2*i+self.a+self.b-2))\n            C = -2*(i + self.a -1)*(i + self.b -1)*(2*i + self.a + self.b)/((2*i)*(i + self.a + self.b)*(2*i + self.a + self.b -2))\n            jacobi[:, :, :, i] = (A*x + B)*jacobi[:, :, :, i-1].clone() + C*jacobi[:, :, :, i-2].clone()\n\n        jacobi = jacobi.permute(0, 2, 3, 1)  \n        y = torch.einsum('bids,iod->bos', jacobi, self.jacobi_coeffs) \n        return y\n\nclass KAN(nn.Module):\n    def __init__(self, input_dim, output_dim, degree, a=ALPHA, b=BETA):\n        super(KAN, self).__init__()\n        self.inputdim = input_dim\n        self.outdim   = output_dim\n        self.a        = a\n        self.b        = b\n        self.degree   = degree\n\n        self.jacobi_coeffs = nn.Parameter(torch.empty(input_dim, output_dim, degree + 1))\n        nn.init.normal_(self.jacobi_coeffs, mean=0.0, std=1/(input_dim * (degree + 1)))\n\n    def forward(self, x):\n        x = torch.reshape(x, (-1, self.inputdim)) \n        x = torch.tanh(x)\n        \n        jacobi = torch.ones(x.shape[0], self.inputdim, self.degree + 1, device=x.device)\n        if self.degree > 0:\n            jacobi[:, :, 1] = ((self.a - self.b) + (self.a + self.b + 2) * x) / 2\n\n        for i in range(2, self.degree + 1):\n            A = (2*i + self.a + self.b - 1)*(2*i + self.a + self.b)/((2*i) * (i + self.a + self.b))\n            B = (2*i + self.a + self.b - 1)*(self.a**2 - self.b**2)/((2*i)*(i + self.a + self.b)*(2*i+self.a+self.b-2))\n            C = -2*(i + self.a -1)*(i + self.b -1)*(2*i + self.a + self.b)/((2*i)*(i + self.a + self.b)*(2*i + self.a + self.b -2))\n            jacobi[:, :, i] = (A*x + B)*jacobi[:, :, i-1].clone() + C*jacobi[:, :, i-2].clone()\n\n        y = torch.einsum('bid,iod->bo', jacobi, self.jacobi_coeffs) \n        y = y.view(-1, self.outdim)\n        return y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:26:44.888062Z","iopub.execute_input":"2025-04-14T16:26:44.888324Z","iopub.status.idle":"2025-04-14T16:26:45.338197Z","shell.execute_reply.started":"2025-04-14T16:26:44.888304Z","shell.execute_reply":"2025-04-14T16:26:45.337281Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Parse Dataset\ndef parse_dataset(num_points=NUM_POINTS):\n    train_points_with_normals = []\n    train_labels = []\n    test_points_with_normals = []\n    test_labels = []\n    class_map = {}\n\n    DATA_DIR = direction\n    folders = glob.glob(os.path.join(DATA_DIR, \"*\"))\n\n    for i, folder in enumerate(folders):\n        print(\"processing class: {}\".format(os.path.basename(folder)))\n        class_map[i] = folder.split(\"/\")[-1]\n        train_files = glob.glob(os.path.join(folder, \"train/*\"))\n        test_files = glob.glob(os.path.join(folder, \"test/*\"))\n\n        for f in train_files:\n            mesh = trimesh.load(f)\n            points, face_indices = mesh.sample(num_points, return_index=True)\n            normals = mesh.face_normals[face_indices]\n            points_with_normals = np.concatenate([points, normals], axis=1)\n            train_points_with_normals.append(points_with_normals)\n            train_labels.append(i)\n\n        for f in test_files:\n            mesh = trimesh.load(f)\n            points, face_indices = mesh.sample(num_points, return_index=True)\n            normals = mesh.face_normals[face_indices]\n            points_with_normals = np.concatenate([points, normals], axis=1)\n            test_points_with_normals.append(points_with_normals)\n            test_labels.append(i)\n\n    train_points = torch.tensor(np.array(train_points_with_normals), dtype=torch.float32)\n    test_points = torch.tensor(np.array(test_points_with_normals), dtype=torch.float32)\n    train_labels = torch.tensor(np.array(train_labels), dtype=torch.long)\n    test_labels = torch.tensor(np.array(test_labels), dtype=torch.long)\n\n    return train_points, test_points, train_labels, test_labels, class_map\n\n# Load Data\ntrain_points, test_points, train_labels, test_labels, CLASS_MAP = parse_dataset(NUM_POINTS)\n\n# Create Datasets and DataLoaders\ntrain_dataset_ssl = PointCloudDataset(train_points, train_labels, ssl=True)\ntrain_loader_ssl = DataLoader(train_dataset_ssl, batch_size=BATCH_SIZE, shuffle=True)\n\ntrain_dataset = PointCloudDataset(train_points, train_labels)\ntest_dataset = PointCloudDataset(test_points, test_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# Training Loop for SSL with Fixed Loss\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = PointNetKANSSL(input_channels=FEATURE, output_channels=NUM_CLASSES, scaling=SCALE).to(device)\ncontrastive_loss = ContrastiveLoss(temperature=0.5)\noptimizer_ssl = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_ssl, T_max=SSL_EPOCHS)\n\nfor epoch in range(SSL_EPOCHS):\n    model.train()\n    running_loss = 0.0\n\n    for points1, points2, _ in train_loader_ssl:\n        points1, points2 = points1.to(device), points2.to(device)\n        points1 = points1.transpose(1, 2)\n        points2 = points2.transpose(1, 2)\n\n        z1 = model(points1, return_projection=True)\n        z2 = model(points2, return_projection=True)\n\n        loss = contrastive_loss(z1, z2)\n\n        optimizer_ssl.zero_grad()\n        loss.backward()\n        optimizer_ssl.step()\n\n        running_loss += loss.item()\n\n    scheduler.step()\n    epoch_loss = running_loss / len(train_loader_ssl)\n    print(f\"SSL Epoch {epoch + 1}/{SSL_EPOCHS}, Loss: {epoch_loss:.4f}, LR: {optimizer_ssl.param_groups[0]['lr']:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:26:45.339016Z","iopub.execute_input":"2025-04-14T16:26:45.339315Z","iopub.status.idle":"2025-04-14T20:14:28.321189Z","shell.execute_reply.started":"2025-04-14T16:26:45.339286Z","shell.execute_reply":"2025-04-14T20:14:28.320356Z"}},"outputs":[{"name":"stdout","text":"processing class: bench\nprocessing class: stairs\nprocessing class: xbox\nprocessing class: sink\nprocessing class: guitar\nprocessing class: night_stand\nprocessing class: laptop\nprocessing class: dresser\nprocessing class: glass_box\nprocessing class: range_hood\nprocessing class: mantel\nprocessing class: bed\nprocessing class: keyboard\nprocessing class: cone\nprocessing class: stool\nprocessing class: cup\nprocessing class: vase\nprocessing class: table\nprocessing class: car\nprocessing class: plant\nprocessing class: door\nprocessing class: airplane\nprocessing class: chair\nprocessing class: desk\nprocessing class: wardrobe\nprocessing class: monitor\nprocessing class: radio\nprocessing class: lamp\nprocessing class: toilet\nprocessing class: bathtub\nprocessing class: bookshelf\nprocessing class: piano\nprocessing class: bowl\nprocessing class: tv_stand\nprocessing class: sofa\nprocessing class: curtain\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trimesh/grouping.py:99: RuntimeWarning: invalid value encountered in cast\n  stacked = np.column_stack(stacked).round().astype(np.int64)\n","output_type":"stream"},{"name":"stdout","text":"processing class: flower_pot\nprocessing class: bottle\nprocessing class: tent\nprocessing class: person\nSSL Epoch 1/300, Loss: 2.7936, LR: 0.001000\nSSL Epoch 2/300, Loss: 2.5263, LR: 0.001000\nSSL Epoch 3/300, Loss: 2.4826, LR: 0.001000\nSSL Epoch 4/300, Loss: 2.4619, LR: 0.001000\nSSL Epoch 5/300, Loss: 2.4735, LR: 0.000999\nSSL Epoch 6/300, Loss: 2.4530, LR: 0.000999\nSSL Epoch 7/300, Loss: 2.4364, LR: 0.000999\nSSL Epoch 8/300, Loss: 2.4675, LR: 0.000998\nSSL Epoch 9/300, Loss: 2.4087, LR: 0.000998\nSSL Epoch 10/300, Loss: 2.3841, LR: 0.000997\nSSL Epoch 11/300, Loss: 2.3951, LR: 0.000997\nSSL Epoch 12/300, Loss: 2.3959, LR: 0.000996\nSSL Epoch 13/300, Loss: 2.4169, LR: 0.000995\nSSL Epoch 14/300, Loss: 2.4026, LR: 0.000995\nSSL Epoch 15/300, Loss: 2.3609, LR: 0.000994\nSSL Epoch 16/300, Loss: 2.3521, LR: 0.000993\nSSL Epoch 17/300, Loss: 2.3517, LR: 0.000992\nSSL Epoch 18/300, Loss: 2.3612, LR: 0.000991\nSSL Epoch 19/300, Loss: 2.4264, LR: 0.000990\nSSL Epoch 20/300, Loss: 2.4240, LR: 0.000989\nSSL Epoch 21/300, Loss: 2.3831, LR: 0.000988\nSSL Epoch 22/300, Loss: 2.3874, LR: 0.000987\nSSL Epoch 23/300, Loss: 2.3713, LR: 0.000986\nSSL Epoch 24/300, Loss: 2.3895, LR: 0.000984\nSSL Epoch 25/300, Loss: 2.3550, LR: 0.000983\nSSL Epoch 26/300, Loss: 2.3535, LR: 0.000982\nSSL Epoch 27/300, Loss: 2.3447, LR: 0.000980\nSSL Epoch 28/300, Loss: 2.3374, LR: 0.000979\nSSL Epoch 29/300, Loss: 2.3330, LR: 0.000977\nSSL Epoch 30/300, Loss: 2.3293, LR: 0.000976\nSSL Epoch 31/300, Loss: 2.3284, LR: 0.000974\nSSL Epoch 32/300, Loss: 2.3268, LR: 0.000972\nSSL Epoch 33/300, Loss: 2.3265, LR: 0.000970\nSSL Epoch 34/300, Loss: 2.3254, LR: 0.000969\nSSL Epoch 35/300, Loss: 2.3777, LR: 0.000967\nSSL Epoch 36/300, Loss: 2.4173, LR: 0.000965\nSSL Epoch 37/300, Loss: 2.3456, LR: 0.000963\nSSL Epoch 38/300, Loss: 2.3302, LR: 0.000961\nSSL Epoch 39/300, Loss: 2.3247, LR: 0.000959\nSSL Epoch 40/300, Loss: 2.3241, LR: 0.000957\nSSL Epoch 41/300, Loss: 2.3165, LR: 0.000955\nSSL Epoch 42/300, Loss: 2.3193, LR: 0.000952\nSSL Epoch 43/300, Loss: 2.3319, LR: 0.000950\nSSL Epoch 44/300, Loss: 2.3251, LR: 0.000948\nSSL Epoch 45/300, Loss: 2.3154, LR: 0.000946\nSSL Epoch 46/300, Loss: 2.3144, LR: 0.000943\nSSL Epoch 47/300, Loss: 2.3119, LR: 0.000941\nSSL Epoch 48/300, Loss: 2.3121, LR: 0.000938\nSSL Epoch 49/300, Loss: 2.3124, LR: 0.000936\nSSL Epoch 50/300, Loss: 2.3104, LR: 0.000933\nSSL Epoch 51/300, Loss: 2.3098, LR: 0.000930\nSSL Epoch 52/300, Loss: 2.3120, LR: 0.000928\nSSL Epoch 53/300, Loss: 2.3101, LR: 0.000925\nSSL Epoch 54/300, Loss: 2.3112, LR: 0.000922\nSSL Epoch 55/300, Loss: 2.3085, LR: 0.000919\nSSL Epoch 56/300, Loss: 2.3089, LR: 0.000916\nSSL Epoch 57/300, Loss: 2.3069, LR: 0.000914\nSSL Epoch 58/300, Loss: 2.3091, LR: 0.000911\nSSL Epoch 59/300, Loss: 2.3106, LR: 0.000908\nSSL Epoch 60/300, Loss: 2.3162, LR: 0.000905\nSSL Epoch 61/300, Loss: 2.3098, LR: 0.000901\nSSL Epoch 62/300, Loss: 2.3094, LR: 0.000898\nSSL Epoch 63/300, Loss: 2.3048, LR: 0.000895\nSSL Epoch 64/300, Loss: 2.3074, LR: 0.000892\nSSL Epoch 65/300, Loss: 2.3046, LR: 0.000889\nSSL Epoch 66/300, Loss: 2.3324, LR: 0.000885\nSSL Epoch 67/300, Loss: 2.3339, LR: 0.000882\nSSL Epoch 68/300, Loss: 2.3133, LR: 0.000878\nSSL Epoch 69/300, Loss: 2.3090, LR: 0.000875\nSSL Epoch 70/300, Loss: 2.3087, LR: 0.000872\nSSL Epoch 71/300, Loss: 2.3265, LR: 0.000868\nSSL Epoch 72/300, Loss: 2.3078, LR: 0.000864\nSSL Epoch 73/300, Loss: 2.3121, LR: 0.000861\nSSL Epoch 74/300, Loss: 2.3058, LR: 0.000857\nSSL Epoch 75/300, Loss: 2.3030, LR: 0.000854\nSSL Epoch 76/300, Loss: 2.3059, LR: 0.000850\nSSL Epoch 77/300, Loss: 2.3040, LR: 0.000846\nSSL Epoch 78/300, Loss: 2.3133, LR: 0.000842\nSSL Epoch 79/300, Loss: 2.3036, LR: 0.000838\nSSL Epoch 80/300, Loss: 2.3058, LR: 0.000835\nSSL Epoch 81/300, Loss: 2.3015, LR: 0.000831\nSSL Epoch 82/300, Loss: 2.2997, LR: 0.000827\nSSL Epoch 83/300, Loss: 2.3000, LR: 0.000823\nSSL Epoch 84/300, Loss: 2.2986, LR: 0.000819\nSSL Epoch 85/300, Loss: 2.2981, LR: 0.000815\nSSL Epoch 86/300, Loss: 2.2984, LR: 0.000811\nSSL Epoch 87/300, Loss: 2.3020, LR: 0.000806\nSSL Epoch 88/300, Loss: 2.3151, LR: 0.000802\nSSL Epoch 89/300, Loss: 2.2989, LR: 0.000798\nSSL Epoch 90/300, Loss: 2.2979, LR: 0.000794\nSSL Epoch 91/300, Loss: 2.3011, LR: 0.000790\nSSL Epoch 92/300, Loss: 2.2978, LR: 0.000785\nSSL Epoch 93/300, Loss: 2.2968, LR: 0.000781\nSSL Epoch 94/300, Loss: 2.2963, LR: 0.000777\nSSL Epoch 95/300, Loss: 2.2962, LR: 0.000772\nSSL Epoch 96/300, Loss: 2.2966, LR: 0.000768\nSSL Epoch 97/300, Loss: 2.2989, LR: 0.000763\nSSL Epoch 98/300, Loss: 2.3135, LR: 0.000759\nSSL Epoch 99/300, Loss: 2.2953, LR: 0.000755\nSSL Epoch 100/300, Loss: 2.2947, LR: 0.000750\nSSL Epoch 101/300, Loss: 2.2921, LR: 0.000745\nSSL Epoch 102/300, Loss: 2.2938, LR: 0.000741\nSSL Epoch 103/300, Loss: 2.2944, LR: 0.000736\nSSL Epoch 104/300, Loss: 2.2944, LR: 0.000732\nSSL Epoch 105/300, Loss: 2.2946, LR: 0.000727\nSSL Epoch 106/300, Loss: 2.2944, LR: 0.000722\nSSL Epoch 107/300, Loss: 2.2944, LR: 0.000718\nSSL Epoch 108/300, Loss: 2.2926, LR: 0.000713\nSSL Epoch 109/300, Loss: 2.2929, LR: 0.000708\nSSL Epoch 110/300, Loss: 2.2993, LR: 0.000703\nSSL Epoch 111/300, Loss: 2.3007, LR: 0.000699\nSSL Epoch 112/300, Loss: 2.2932, LR: 0.000694\nSSL Epoch 113/300, Loss: 2.2960, LR: 0.000689\nSSL Epoch 114/300, Loss: 2.2931, LR: 0.000684\nSSL Epoch 115/300, Loss: 2.2914, LR: 0.000679\nSSL Epoch 116/300, Loss: 2.2916, LR: 0.000674\nSSL Epoch 117/300, Loss: 2.2933, LR: 0.000669\nSSL Epoch 118/300, Loss: 2.2914, LR: 0.000664\nSSL Epoch 119/300, Loss: 2.2901, LR: 0.000659\nSSL Epoch 120/300, Loss: 2.2905, LR: 0.000655\nSSL Epoch 121/300, Loss: 2.2905, LR: 0.000650\nSSL Epoch 122/300, Loss: 2.2922, LR: 0.000645\nSSL Epoch 123/300, Loss: 2.2895, LR: 0.000639\nSSL Epoch 124/300, Loss: 2.2890, LR: 0.000634\nSSL Epoch 125/300, Loss: 2.2908, LR: 0.000629\nSSL Epoch 126/300, Loss: 2.2905, LR: 0.000624\nSSL Epoch 127/300, Loss: 2.2894, LR: 0.000619\nSSL Epoch 128/300, Loss: 2.2897, LR: 0.000614\nSSL Epoch 129/300, Loss: 2.2885, LR: 0.000609\nSSL Epoch 130/300, Loss: 2.2877, LR: 0.000604\nSSL Epoch 131/300, Loss: 2.2898, LR: 0.000599\nSSL Epoch 132/300, Loss: 2.2882, LR: 0.000594\nSSL Epoch 133/300, Loss: 2.2884, LR: 0.000589\nSSL Epoch 134/300, Loss: 2.2887, LR: 0.000583\nSSL Epoch 135/300, Loss: 2.2879, LR: 0.000578\nSSL Epoch 136/300, Loss: 2.2898, LR: 0.000573\nSSL Epoch 137/300, Loss: 2.2886, LR: 0.000568\nSSL Epoch 138/300, Loss: 2.2881, LR: 0.000563\nSSL Epoch 139/300, Loss: 2.2867, LR: 0.000557\nSSL Epoch 140/300, Loss: 2.2879, LR: 0.000552\nSSL Epoch 141/300, Loss: 2.2883, LR: 0.000547\nSSL Epoch 142/300, Loss: 2.2885, LR: 0.000542\nSSL Epoch 143/300, Loss: 2.2868, LR: 0.000537\nSSL Epoch 144/300, Loss: 2.2868, LR: 0.000531\nSSL Epoch 145/300, Loss: 2.2870, LR: 0.000526\nSSL Epoch 146/300, Loss: 2.2880, LR: 0.000521\nSSL Epoch 147/300, Loss: 2.2869, LR: 0.000516\nSSL Epoch 148/300, Loss: 2.2877, LR: 0.000510\nSSL Epoch 149/300, Loss: 2.2867, LR: 0.000505\nSSL Epoch 150/300, Loss: 2.2862, LR: 0.000500\nSSL Epoch 151/300, Loss: 2.2862, LR: 0.000495\nSSL Epoch 152/300, Loss: 2.2865, LR: 0.000490\nSSL Epoch 153/300, Loss: 2.2860, LR: 0.000484\nSSL Epoch 154/300, Loss: 2.2859, LR: 0.000479\nSSL Epoch 155/300, Loss: 2.2851, LR: 0.000474\nSSL Epoch 156/300, Loss: 2.2852, LR: 0.000469\nSSL Epoch 157/300, Loss: 2.2863, LR: 0.000463\nSSL Epoch 158/300, Loss: 2.2869, LR: 0.000458\nSSL Epoch 159/300, Loss: 2.2869, LR: 0.000453\nSSL Epoch 160/300, Loss: 2.2851, LR: 0.000448\nSSL Epoch 161/300, Loss: 2.2858, LR: 0.000443\nSSL Epoch 162/300, Loss: 2.2843, LR: 0.000437\nSSL Epoch 163/300, Loss: 2.2847, LR: 0.000432\nSSL Epoch 164/300, Loss: 2.2848, LR: 0.000427\nSSL Epoch 165/300, Loss: 2.2841, LR: 0.000422\nSSL Epoch 166/300, Loss: 2.2855, LR: 0.000417\nSSL Epoch 167/300, Loss: 2.2845, LR: 0.000411\nSSL Epoch 168/300, Loss: 2.2845, LR: 0.000406\nSSL Epoch 169/300, Loss: 2.2838, LR: 0.000401\nSSL Epoch 170/300, Loss: 2.2839, LR: 0.000396\nSSL Epoch 171/300, Loss: 2.2843, LR: 0.000391\nSSL Epoch 172/300, Loss: 2.2835, LR: 0.000386\nSSL Epoch 173/300, Loss: 2.2837, LR: 0.000381\nSSL Epoch 174/300, Loss: 2.2846, LR: 0.000376\nSSL Epoch 175/300, Loss: 2.2832, LR: 0.000371\nSSL Epoch 176/300, Loss: 2.2831, LR: 0.000366\nSSL Epoch 177/300, Loss: 2.2827, LR: 0.000361\nSSL Epoch 178/300, Loss: 2.2839, LR: 0.000355\nSSL Epoch 179/300, Loss: 2.2890, LR: 0.000350\nSSL Epoch 180/300, Loss: 2.2827, LR: 0.000345\nSSL Epoch 181/300, Loss: 2.2823, LR: 0.000341\nSSL Epoch 182/300, Loss: 2.2834, LR: 0.000336\nSSL Epoch 183/300, Loss: 2.2825, LR: 0.000331\nSSL Epoch 184/300, Loss: 2.2822, LR: 0.000326\nSSL Epoch 185/300, Loss: 2.2818, LR: 0.000321\nSSL Epoch 186/300, Loss: 2.2815, LR: 0.000316\nSSL Epoch 187/300, Loss: 2.2833, LR: 0.000311\nSSL Epoch 188/300, Loss: 2.2829, LR: 0.000306\nSSL Epoch 189/300, Loss: 2.2830, LR: 0.000301\nSSL Epoch 190/300, Loss: 2.2823, LR: 0.000297\nSSL Epoch 191/300, Loss: 2.2821, LR: 0.000292\nSSL Epoch 192/300, Loss: 2.2821, LR: 0.000287\nSSL Epoch 193/300, Loss: 2.2808, LR: 0.000282\nSSL Epoch 194/300, Loss: 2.2818, LR: 0.000278\nSSL Epoch 195/300, Loss: 2.2827, LR: 0.000273\nSSL Epoch 196/300, Loss: 2.2815, LR: 0.000268\nSSL Epoch 197/300, Loss: 2.2818, LR: 0.000264\nSSL Epoch 198/300, Loss: 2.2819, LR: 0.000259\nSSL Epoch 199/300, Loss: 2.2816, LR: 0.000255\nSSL Epoch 200/300, Loss: 2.2817, LR: 0.000250\nSSL Epoch 201/300, Loss: 2.2823, LR: 0.000245\nSSL Epoch 202/300, Loss: 2.2816, LR: 0.000241\nSSL Epoch 203/300, Loss: 2.2811, LR: 0.000237\nSSL Epoch 204/300, Loss: 2.2804, LR: 0.000232\nSSL Epoch 205/300, Loss: 2.2813, LR: 0.000228\nSSL Epoch 206/300, Loss: 2.2803, LR: 0.000223\nSSL Epoch 207/300, Loss: 2.2813, LR: 0.000219\nSSL Epoch 208/300, Loss: 2.2798, LR: 0.000215\nSSL Epoch 209/300, Loss: 2.2808, LR: 0.000210\nSSL Epoch 210/300, Loss: 2.2810, LR: 0.000206\nSSL Epoch 211/300, Loss: 2.2804, LR: 0.000202\nSSL Epoch 212/300, Loss: 2.2808, LR: 0.000198\nSSL Epoch 213/300, Loss: 2.2803, LR: 0.000194\nSSL Epoch 214/300, Loss: 2.2797, LR: 0.000189\nSSL Epoch 215/300, Loss: 2.2799, LR: 0.000185\nSSL Epoch 216/300, Loss: 2.2802, LR: 0.000181\nSSL Epoch 217/300, Loss: 2.2805, LR: 0.000177\nSSL Epoch 218/300, Loss: 2.2799, LR: 0.000173\nSSL Epoch 219/300, Loss: 2.2804, LR: 0.000169\nSSL Epoch 220/300, Loss: 2.2807, LR: 0.000165\nSSL Epoch 221/300, Loss: 2.2804, LR: 0.000162\nSSL Epoch 222/300, Loss: 2.2803, LR: 0.000158\nSSL Epoch 223/300, Loss: 2.2803, LR: 0.000154\nSSL Epoch 224/300, Loss: 2.2794, LR: 0.000150\nSSL Epoch 225/300, Loss: 2.2787, LR: 0.000146\nSSL Epoch 226/300, Loss: 2.2799, LR: 0.000143\nSSL Epoch 227/300, Loss: 2.2789, LR: 0.000139\nSSL Epoch 228/300, Loss: 2.2802, LR: 0.000136\nSSL Epoch 229/300, Loss: 2.2798, LR: 0.000132\nSSL Epoch 230/300, Loss: 2.2795, LR: 0.000128\nSSL Epoch 231/300, Loss: 2.2786, LR: 0.000125\nSSL Epoch 232/300, Loss: 2.2789, LR: 0.000122\nSSL Epoch 233/300, Loss: 2.2788, LR: 0.000118\nSSL Epoch 234/300, Loss: 2.2788, LR: 0.000115\nSSL Epoch 235/300, Loss: 2.2786, LR: 0.000111\nSSL Epoch 236/300, Loss: 2.2784, LR: 0.000108\nSSL Epoch 237/300, Loss: 2.2797, LR: 0.000105\nSSL Epoch 238/300, Loss: 2.2785, LR: 0.000102\nSSL Epoch 239/300, Loss: 2.2781, LR: 0.000099\nSSL Epoch 240/300, Loss: 2.2785, LR: 0.000095\nSSL Epoch 241/300, Loss: 2.2794, LR: 0.000092\nSSL Epoch 242/300, Loss: 2.2792, LR: 0.000089\nSSL Epoch 243/300, Loss: 2.2789, LR: 0.000086\nSSL Epoch 244/300, Loss: 2.2786, LR: 0.000084\nSSL Epoch 245/300, Loss: 2.2786, LR: 0.000081\nSSL Epoch 246/300, Loss: 2.2795, LR: 0.000078\nSSL Epoch 247/300, Loss: 2.2786, LR: 0.000075\nSSL Epoch 248/300, Loss: 2.2794, LR: 0.000072\nSSL Epoch 249/300, Loss: 2.2788, LR: 0.000070\nSSL Epoch 250/300, Loss: 2.2780, LR: 0.000067\nSSL Epoch 251/300, Loss: 2.2780, LR: 0.000064\nSSL Epoch 252/300, Loss: 2.2773, LR: 0.000062\nSSL Epoch 253/300, Loss: 2.2790, LR: 0.000059\nSSL Epoch 254/300, Loss: 2.2782, LR: 0.000057\nSSL Epoch 255/300, Loss: 2.2784, LR: 0.000054\nSSL Epoch 256/300, Loss: 2.2777, LR: 0.000052\nSSL Epoch 257/300, Loss: 2.2786, LR: 0.000050\nSSL Epoch 258/300, Loss: 2.2783, LR: 0.000048\nSSL Epoch 259/300, Loss: 2.2781, LR: 0.000045\nSSL Epoch 260/300, Loss: 2.2778, LR: 0.000043\nSSL Epoch 261/300, Loss: 2.2781, LR: 0.000041\nSSL Epoch 262/300, Loss: 2.2787, LR: 0.000039\nSSL Epoch 263/300, Loss: 2.2785, LR: 0.000037\nSSL Epoch 264/300, Loss: 2.2782, LR: 0.000035\nSSL Epoch 265/300, Loss: 2.2782, LR: 0.000033\nSSL Epoch 266/300, Loss: 2.2782, LR: 0.000031\nSSL Epoch 267/300, Loss: 2.2788, LR: 0.000030\nSSL Epoch 268/300, Loss: 2.2780, LR: 0.000028\nSSL Epoch 269/300, Loss: 2.2781, LR: 0.000026\nSSL Epoch 270/300, Loss: 2.2778, LR: 0.000024\nSSL Epoch 271/300, Loss: 2.2771, LR: 0.000023\nSSL Epoch 272/300, Loss: 2.2777, LR: 0.000021\nSSL Epoch 273/300, Loss: 2.2776, LR: 0.000020\nSSL Epoch 274/300, Loss: 2.2778, LR: 0.000018\nSSL Epoch 275/300, Loss: 2.2782, LR: 0.000017\nSSL Epoch 276/300, Loss: 2.2775, LR: 0.000016\nSSL Epoch 277/300, Loss: 2.2774, LR: 0.000014\nSSL Epoch 278/300, Loss: 2.2772, LR: 0.000013\nSSL Epoch 279/300, Loss: 2.2772, LR: 0.000012\nSSL Epoch 280/300, Loss: 2.2790, LR: 0.000011\nSSL Epoch 281/300, Loss: 2.2779, LR: 0.000010\nSSL Epoch 282/300, Loss: 2.2772, LR: 0.000009\nSSL Epoch 283/300, Loss: 2.2790, LR: 0.000008\nSSL Epoch 284/300, Loss: 2.2774, LR: 0.000007\nSSL Epoch 285/300, Loss: 2.2776, LR: 0.000006\nSSL Epoch 286/300, Loss: 2.2781, LR: 0.000005\nSSL Epoch 287/300, Loss: 2.2773, LR: 0.000005\nSSL Epoch 288/300, Loss: 2.2765, LR: 0.000004\nSSL Epoch 289/300, Loss: 2.2774, LR: 0.000003\nSSL Epoch 290/300, Loss: 2.2771, LR: 0.000003\nSSL Epoch 291/300, Loss: 2.2780, LR: 0.000002\nSSL Epoch 292/300, Loss: 2.2787, LR: 0.000002\nSSL Epoch 293/300, Loss: 2.2773, LR: 0.000001\nSSL Epoch 294/300, Loss: 2.2779, LR: 0.000001\nSSL Epoch 295/300, Loss: 2.2790, LR: 0.000001\nSSL Epoch 296/300, Loss: 2.2782, LR: 0.000000\nSSL Epoch 297/300, Loss: 2.2771, LR: 0.000000\nSSL Epoch 298/300, Loss: 2.2781, LR: 0.000000\nSSL Epoch 299/300, Loss: 2.2770, LR: 0.000000\nSSL Epoch 300/300, Loss: 2.2777, LR: 0.000000\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# Fine-Tuning for Classification\nmodel.projection_head = nn.Linear(NUM_CLASSES, NUM_CLASSES).to(device)  # Replace projection head\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nprint(\"Starting Fine-Tuning for Classification...\")\nfor epoch in range(MAX_EPOCHS):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for points, labels in train_loader:\n        points, labels = points.to(device), labels.to(device)\n        points = points.transpose(1, 2)\n\n        outputs = model(points)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * points.size(0)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    epoch_accuracy = 100 * correct / total\n    print(f\"Epoch {epoch + 1}/{MAX_EPOCHS}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n\n# Evaluation\nmodel.eval()\nval_loss = 0.0\nval_correct = 0\nval_total = 0\n\nwith torch.no_grad():\n    for points, labels in test_loader:\n        points, labels = points.to(device), labels.to(device)\n        points = points.transpose(1, 2)\n\n        outputs = model(points)\n        loss = criterion(outputs, labels)\n\n        val_loss += loss.item() * points.size(0)\n        _, predicted = torch.max(outputs, 1)\n        val_total += labels.size(0)\n        val_correct += (predicted == labels).sum().item()\n\nval_loss /= len(test_loader.dataset)\nval_accuracy = 100 * val_correct / val_total\nprint(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_accuracy:.2f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T20:14:28.322118Z","iopub.execute_input":"2025-04-14T20:14:28.322622Z","iopub.status.idle":"2025-04-14T21:34:19.899596Z","shell.execute_reply.started":"2025-04-14T20:14:28.322599Z","shell.execute_reply":"2025-04-14T21:34:19.898656Z"}},"outputs":[{"name":"stdout","text":"Starting Fine-Tuning for Classification...\nEpoch 1/300, Loss: 152.5357, Accuracy: 28.50%\nEpoch 2/300, Loss: 12.4527, Accuracy: 63.13%\nEpoch 3/300, Loss: 5.4092, Accuracy: 74.21%\nEpoch 4/300, Loss: 3.5992, Accuracy: 78.68%\nEpoch 5/300, Loss: 2.7902, Accuracy: 81.60%\nEpoch 6/300, Loss: 2.1975, Accuracy: 83.96%\nEpoch 7/300, Loss: 1.9012, Accuracy: 84.85%\nEpoch 8/300, Loss: 1.6425, Accuracy: 86.05%\nEpoch 9/300, Loss: 1.4757, Accuracy: 87.36%\nEpoch 10/300, Loss: 1.1452, Accuracy: 88.64%\nEpoch 11/300, Loss: 1.0635, Accuracy: 89.33%\nEpoch 12/300, Loss: 1.2547, Accuracy: 88.90%\nEpoch 13/300, Loss: 1.0798, Accuracy: 89.61%\nEpoch 14/300, Loss: 0.8637, Accuracy: 91.18%\nEpoch 15/300, Loss: 0.9536, Accuracy: 90.55%\nEpoch 16/300, Loss: 0.8049, Accuracy: 91.82%\nEpoch 17/300, Loss: 0.6954, Accuracy: 92.25%\nEpoch 18/300, Loss: 0.8583, Accuracy: 91.14%\nEpoch 19/300, Loss: 0.8304, Accuracy: 91.53%\nEpoch 20/300, Loss: 0.7775, Accuracy: 92.17%\nEpoch 21/300, Loss: 0.8624, Accuracy: 92.03%\nEpoch 22/300, Loss: 0.7381, Accuracy: 92.67%\nEpoch 23/300, Loss: 0.6563, Accuracy: 93.21%\nEpoch 24/300, Loss: 0.6511, Accuracy: 93.45%\nEpoch 25/300, Loss: 0.7685, Accuracy: 92.43%\nEpoch 26/300, Loss: 0.6083, Accuracy: 93.76%\nEpoch 27/300, Loss: 0.7654, Accuracy: 92.70%\nEpoch 28/300, Loss: 0.7521, Accuracy: 93.20%\nEpoch 29/300, Loss: 0.8701, Accuracy: 92.74%\nEpoch 30/300, Loss: 0.6219, Accuracy: 94.09%\nEpoch 31/300, Loss: 0.4930, Accuracy: 94.94%\nEpoch 32/300, Loss: 0.6164, Accuracy: 94.51%\nEpoch 33/300, Loss: 0.5023, Accuracy: 94.83%\nEpoch 34/300, Loss: 0.5565, Accuracy: 94.50%\nEpoch 35/300, Loss: 0.5179, Accuracy: 95.12%\nEpoch 36/300, Loss: 0.7781, Accuracy: 93.70%\nEpoch 37/300, Loss: 0.5622, Accuracy: 94.97%\nEpoch 38/300, Loss: 0.4551, Accuracy: 95.65%\nEpoch 39/300, Loss: 0.4079, Accuracy: 95.88%\nEpoch 40/300, Loss: 0.4295, Accuracy: 95.70%\nEpoch 41/300, Loss: 0.4896, Accuracy: 95.49%\nEpoch 42/300, Loss: 0.5258, Accuracy: 95.09%\nEpoch 43/300, Loss: 0.6818, Accuracy: 94.48%\nEpoch 44/300, Loss: 0.4903, Accuracy: 95.38%\nEpoch 45/300, Loss: 0.3433, Accuracy: 96.51%\nEpoch 46/300, Loss: 0.4223, Accuracy: 96.16%\nEpoch 47/300, Loss: 0.5511, Accuracy: 95.35%\nEpoch 48/300, Loss: 0.6138, Accuracy: 95.30%\nEpoch 49/300, Loss: 0.5023, Accuracy: 95.84%\nEpoch 50/300, Loss: 0.4680, Accuracy: 96.14%\nEpoch 51/300, Loss: 0.3377, Accuracy: 96.67%\nEpoch 52/300, Loss: 0.3093, Accuracy: 96.86%\nEpoch 53/300, Loss: 0.5200, Accuracy: 95.61%\nEpoch 54/300, Loss: 0.4702, Accuracy: 96.21%\nEpoch 55/300, Loss: 0.3818, Accuracy: 96.69%\nEpoch 56/300, Loss: 0.7278, Accuracy: 95.06%\nEpoch 57/300, Loss: 0.4129, Accuracy: 96.51%\nEpoch 58/300, Loss: 0.4809, Accuracy: 96.31%\nEpoch 59/300, Loss: 0.4749, Accuracy: 96.58%\nEpoch 60/300, Loss: 0.2914, Accuracy: 97.46%\nEpoch 61/300, Loss: 0.3153, Accuracy: 97.34%\nEpoch 62/300, Loss: 0.2135, Accuracy: 97.75%\nEpoch 63/300, Loss: 0.4716, Accuracy: 96.45%\nEpoch 64/300, Loss: 0.5348, Accuracy: 96.06%\nEpoch 65/300, Loss: 0.4836, Accuracy: 96.40%\nEpoch 66/300, Loss: 0.4187, Accuracy: 96.70%\nEpoch 67/300, Loss: 0.5600, Accuracy: 96.21%\nEpoch 68/300, Loss: 0.3589, Accuracy: 97.41%\nEpoch 69/300, Loss: 0.2826, Accuracy: 97.74%\nEpoch 70/300, Loss: 0.2557, Accuracy: 97.80%\nEpoch 71/300, Loss: 0.1792, Accuracy: 98.23%\nEpoch 72/300, Loss: 0.3673, Accuracy: 97.34%\nEpoch 73/300, Loss: 0.4897, Accuracy: 96.34%\nEpoch 74/300, Loss: 0.4225, Accuracy: 96.69%\nEpoch 75/300, Loss: 0.2952, Accuracy: 97.54%\nEpoch 76/300, Loss: 0.2324, Accuracy: 97.93%\nEpoch 77/300, Loss: 0.3331, Accuracy: 97.45%\nEpoch 78/300, Loss: 0.3254, Accuracy: 97.30%\nEpoch 79/300, Loss: 0.2992, Accuracy: 97.54%\nEpoch 80/300, Loss: 0.3967, Accuracy: 97.06%\nEpoch 81/300, Loss: 0.1901, Accuracy: 98.34%\nEpoch 82/300, Loss: 0.3390, Accuracy: 97.38%\nEpoch 83/300, Loss: 0.1607, Accuracy: 98.53%\nEpoch 84/300, Loss: 0.2468, Accuracy: 97.85%\nEpoch 85/300, Loss: 0.3272, Accuracy: 97.48%\nEpoch 86/300, Loss: 0.3060, Accuracy: 97.58%\nEpoch 87/300, Loss: 0.2670, Accuracy: 97.73%\nEpoch 88/300, Loss: 0.2180, Accuracy: 98.20%\nEpoch 89/300, Loss: 0.2464, Accuracy: 98.08%\nEpoch 90/300, Loss: 0.2790, Accuracy: 97.88%\nEpoch 91/300, Loss: 0.2980, Accuracy: 98.06%\nEpoch 92/300, Loss: 0.7360, Accuracy: 96.20%\nEpoch 93/300, Loss: 0.5966, Accuracy: 96.62%\nEpoch 94/300, Loss: 0.2290, Accuracy: 98.11%\nEpoch 95/300, Loss: 0.2880, Accuracy: 97.69%\nEpoch 96/300, Loss: 0.4651, Accuracy: 97.19%\nEpoch 97/300, Loss: 0.2954, Accuracy: 97.93%\nEpoch 98/300, Loss: 0.2698, Accuracy: 97.93%\nEpoch 99/300, Loss: 0.2233, Accuracy: 98.06%\nEpoch 100/300, Loss: 0.4136, Accuracy: 97.26%\nEpoch 101/300, Loss: 0.1808, Accuracy: 98.50%\nEpoch 102/300, Loss: 0.2159, Accuracy: 98.29%\nEpoch 103/300, Loss: 0.1953, Accuracy: 98.31%\nEpoch 104/300, Loss: 0.2424, Accuracy: 98.24%\nEpoch 105/300, Loss: 0.2774, Accuracy: 98.07%\nEpoch 106/300, Loss: 0.1773, Accuracy: 98.58%\nEpoch 107/300, Loss: 0.3146, Accuracy: 97.96%\nEpoch 108/300, Loss: 0.1931, Accuracy: 98.56%\nEpoch 109/300, Loss: 0.5571, Accuracy: 96.84%\nEpoch 110/300, Loss: 0.3035, Accuracy: 97.98%\nEpoch 111/300, Loss: 0.2902, Accuracy: 98.10%\nEpoch 112/300, Loss: 0.2155, Accuracy: 98.51%\nEpoch 113/300, Loss: 0.1425, Accuracy: 98.73%\nEpoch 114/300, Loss: 0.2770, Accuracy: 97.97%\nEpoch 115/300, Loss: 0.2889, Accuracy: 98.09%\nEpoch 116/300, Loss: 0.1778, Accuracy: 98.66%\nEpoch 117/300, Loss: 0.1921, Accuracy: 98.81%\nEpoch 118/300, Loss: 0.2425, Accuracy: 98.35%\nEpoch 119/300, Loss: 0.1159, Accuracy: 98.90%\nEpoch 120/300, Loss: 0.1285, Accuracy: 98.90%\nEpoch 121/300, Loss: 0.2408, Accuracy: 98.29%\nEpoch 122/300, Loss: 0.2881, Accuracy: 97.98%\nEpoch 123/300, Loss: 0.1282, Accuracy: 98.85%\nEpoch 124/300, Loss: 0.2806, Accuracy: 98.19%\nEpoch 125/300, Loss: 0.1962, Accuracy: 98.57%\nEpoch 126/300, Loss: 0.1493, Accuracy: 98.82%\nEpoch 127/300, Loss: 0.1164, Accuracy: 99.03%\nEpoch 128/300, Loss: 0.2787, Accuracy: 98.32%\nEpoch 129/300, Loss: 0.2408, Accuracy: 98.39%\nEpoch 130/300, Loss: 0.3356, Accuracy: 98.08%\nEpoch 131/300, Loss: 0.2067, Accuracy: 98.53%\nEpoch 132/300, Loss: 0.2054, Accuracy: 98.35%\nEpoch 133/300, Loss: 0.2808, Accuracy: 98.06%\nEpoch 134/300, Loss: 0.2251, Accuracy: 98.36%\nEpoch 135/300, Loss: 0.1579, Accuracy: 98.88%\nEpoch 136/300, Loss: 0.1648, Accuracy: 98.80%\nEpoch 137/300, Loss: 0.1796, Accuracy: 98.73%\nEpoch 138/300, Loss: 0.2865, Accuracy: 98.36%\nEpoch 139/300, Loss: 0.2265, Accuracy: 98.34%\nEpoch 140/300, Loss: 0.2958, Accuracy: 98.33%\nEpoch 141/300, Loss: 0.3630, Accuracy: 97.94%\nEpoch 142/300, Loss: 0.3129, Accuracy: 98.15%\nEpoch 143/300, Loss: 0.2740, Accuracy: 98.60%\nEpoch 144/300, Loss: 0.1055, Accuracy: 99.09%\nEpoch 145/300, Loss: 0.1390, Accuracy: 99.13%\nEpoch 146/300, Loss: 0.1272, Accuracy: 98.93%\nEpoch 147/300, Loss: 0.1243, Accuracy: 99.19%\nEpoch 148/300, Loss: 0.1246, Accuracy: 99.10%\nEpoch 149/300, Loss: 0.2243, Accuracy: 98.50%\nEpoch 150/300, Loss: 0.3133, Accuracy: 97.87%\nEpoch 151/300, Loss: 0.2692, Accuracy: 98.44%\nEpoch 152/300, Loss: 0.2723, Accuracy: 98.53%\nEpoch 153/300, Loss: 0.1957, Accuracy: 98.68%\nEpoch 154/300, Loss: 0.2186, Accuracy: 98.77%\nEpoch 155/300, Loss: 0.1845, Accuracy: 98.87%\nEpoch 156/300, Loss: 0.0965, Accuracy: 99.20%\nEpoch 157/300, Loss: 0.0903, Accuracy: 99.33%\nEpoch 158/300, Loss: 0.0968, Accuracy: 99.08%\nEpoch 159/300, Loss: 0.2969, Accuracy: 98.26%\nEpoch 160/300, Loss: 0.2948, Accuracy: 98.37%\nEpoch 161/300, Loss: 0.1922, Accuracy: 98.80%\nEpoch 162/300, Loss: 0.1623, Accuracy: 98.94%\nEpoch 163/300, Loss: 0.2413, Accuracy: 98.65%\nEpoch 164/300, Loss: 0.1236, Accuracy: 99.09%\nEpoch 165/300, Loss: 0.1568, Accuracy: 98.98%\nEpoch 166/300, Loss: 0.1864, Accuracy: 98.74%\nEpoch 167/300, Loss: 0.2092, Accuracy: 98.81%\nEpoch 168/300, Loss: 0.3749, Accuracy: 98.27%\nEpoch 169/300, Loss: 0.1737, Accuracy: 98.89%\nEpoch 170/300, Loss: 0.1389, Accuracy: 98.96%\nEpoch 171/300, Loss: 0.1376, Accuracy: 99.16%\nEpoch 172/300, Loss: 0.3151, Accuracy: 98.50%\nEpoch 173/300, Loss: 0.0924, Accuracy: 99.35%\nEpoch 174/300, Loss: 0.1738, Accuracy: 98.95%\nEpoch 175/300, Loss: 0.0894, Accuracy: 99.40%\nEpoch 176/300, Loss: 0.2360, Accuracy: 98.65%\nEpoch 177/300, Loss: 0.1958, Accuracy: 98.84%\nEpoch 178/300, Loss: 0.2275, Accuracy: 98.62%\nEpoch 179/300, Loss: 0.2120, Accuracy: 98.77%\nEpoch 180/300, Loss: 0.3975, Accuracy: 98.18%\nEpoch 181/300, Loss: 0.2431, Accuracy: 98.60%\nEpoch 182/300, Loss: 0.1638, Accuracy: 99.05%\nEpoch 183/300, Loss: 0.2272, Accuracy: 98.51%\nEpoch 184/300, Loss: 0.0772, Accuracy: 99.55%\nEpoch 185/300, Loss: 0.0478, Accuracy: 99.67%\nEpoch 186/300, Loss: 0.1452, Accuracy: 99.02%\nEpoch 187/300, Loss: 0.2265, Accuracy: 98.77%\nEpoch 188/300, Loss: 0.2346, Accuracy: 98.70%\nEpoch 189/300, Loss: 0.2535, Accuracy: 98.45%\nEpoch 190/300, Loss: 0.1924, Accuracy: 98.93%\nEpoch 191/300, Loss: 0.1127, Accuracy: 99.26%\nEpoch 192/300, Loss: 0.0806, Accuracy: 99.29%\nEpoch 193/300, Loss: 0.1152, Accuracy: 99.20%\nEpoch 194/300, Loss: 0.2088, Accuracy: 98.82%\nEpoch 195/300, Loss: 0.2401, Accuracy: 98.67%\nEpoch 196/300, Loss: 0.1913, Accuracy: 98.92%\nEpoch 197/300, Loss: 0.0892, Accuracy: 99.32%\nEpoch 198/300, Loss: 0.1672, Accuracy: 99.10%\nEpoch 199/300, Loss: 0.1537, Accuracy: 98.93%\nEpoch 200/300, Loss: 0.2218, Accuracy: 98.77%\nEpoch 201/300, Loss: 0.1647, Accuracy: 99.07%\nEpoch 202/300, Loss: 0.1079, Accuracy: 99.22%\nEpoch 203/300, Loss: 0.1256, Accuracy: 99.20%\nEpoch 204/300, Loss: 0.1611, Accuracy: 99.06%\nEpoch 205/300, Loss: 0.1195, Accuracy: 99.28%\nEpoch 206/300, Loss: 0.2079, Accuracy: 98.88%\nEpoch 207/300, Loss: 0.1225, Accuracy: 99.31%\nEpoch 208/300, Loss: 0.1973, Accuracy: 98.75%\nEpoch 209/300, Loss: 0.2017, Accuracy: 98.98%\nEpoch 210/300, Loss: 0.1784, Accuracy: 98.85%\nEpoch 211/300, Loss: 0.1142, Accuracy: 99.34%\nEpoch 212/300, Loss: 0.0918, Accuracy: 99.44%\nEpoch 213/300, Loss: 0.1123, Accuracy: 99.28%\nEpoch 214/300, Loss: 0.1538, Accuracy: 99.09%\nEpoch 215/300, Loss: 0.1355, Accuracy: 99.15%\nEpoch 216/300, Loss: 0.2207, Accuracy: 98.84%\nEpoch 217/300, Loss: 0.1348, Accuracy: 99.20%\nEpoch 218/300, Loss: 0.1284, Accuracy: 99.11%\nEpoch 219/300, Loss: 0.1018, Accuracy: 99.35%\nEpoch 220/300, Loss: 0.1682, Accuracy: 99.03%\nEpoch 221/300, Loss: 0.1209, Accuracy: 99.36%\nEpoch 222/300, Loss: 0.2208, Accuracy: 98.90%\nEpoch 223/300, Loss: 0.1284, Accuracy: 99.24%\nEpoch 224/300, Loss: 0.0659, Accuracy: 99.60%\nEpoch 225/300, Loss: 0.0843, Accuracy: 99.41%\nEpoch 226/300, Loss: 0.1656, Accuracy: 99.21%\nEpoch 227/300, Loss: 0.2723, Accuracy: 98.64%\nEpoch 228/300, Loss: 0.2076, Accuracy: 98.89%\nEpoch 229/300, Loss: 0.1645, Accuracy: 99.15%\nEpoch 230/300, Loss: 0.1691, Accuracy: 99.08%\nEpoch 231/300, Loss: 0.1249, Accuracy: 99.28%\nEpoch 232/300, Loss: 0.1107, Accuracy: 99.39%\nEpoch 233/300, Loss: 0.1608, Accuracy: 99.17%\nEpoch 234/300, Loss: 0.1134, Accuracy: 99.32%\nEpoch 235/300, Loss: 0.0970, Accuracy: 99.28%\nEpoch 236/300, Loss: 0.2313, Accuracy: 98.83%\nEpoch 237/300, Loss: 0.0725, Accuracy: 99.50%\nEpoch 238/300, Loss: 0.1481, Accuracy: 99.24%\nEpoch 239/300, Loss: 0.0610, Accuracy: 99.61%\nEpoch 240/300, Loss: 0.1927, Accuracy: 98.86%\nEpoch 241/300, Loss: 0.1617, Accuracy: 99.20%\nEpoch 242/300, Loss: 0.1460, Accuracy: 99.01%\nEpoch 243/300, Loss: 0.0878, Accuracy: 99.39%\nEpoch 244/300, Loss: 0.1455, Accuracy: 99.24%\nEpoch 245/300, Loss: 0.1118, Accuracy: 99.37%\nEpoch 246/300, Loss: 0.2717, Accuracy: 98.82%\nEpoch 247/300, Loss: 0.0960, Accuracy: 99.40%\nEpoch 248/300, Loss: 0.0900, Accuracy: 99.49%\nEpoch 249/300, Loss: 0.3032, Accuracy: 98.49%\nEpoch 250/300, Loss: 0.1596, Accuracy: 99.08%\nEpoch 251/300, Loss: 0.0672, Accuracy: 99.56%\nEpoch 252/300, Loss: 0.1139, Accuracy: 99.24%\nEpoch 253/300, Loss: 0.2853, Accuracy: 98.59%\nEpoch 254/300, Loss: 0.2030, Accuracy: 98.88%\nEpoch 255/300, Loss: 0.1510, Accuracy: 99.18%\nEpoch 256/300, Loss: 0.0866, Accuracy: 99.48%\nEpoch 257/300, Loss: 0.0621, Accuracy: 99.60%\nEpoch 258/300, Loss: 0.1328, Accuracy: 99.22%\nEpoch 259/300, Loss: 0.2186, Accuracy: 99.07%\nEpoch 260/300, Loss: 0.1184, Accuracy: 99.42%\nEpoch 261/300, Loss: 0.1344, Accuracy: 99.29%\nEpoch 262/300, Loss: 0.1890, Accuracy: 99.20%\nEpoch 263/300, Loss: 0.1399, Accuracy: 99.27%\nEpoch 264/300, Loss: 0.0722, Accuracy: 99.47%\nEpoch 265/300, Loss: 0.1248, Accuracy: 99.25%\nEpoch 266/300, Loss: 0.2216, Accuracy: 98.89%\nEpoch 267/300, Loss: 0.1154, Accuracy: 99.31%\nEpoch 268/300, Loss: 0.1223, Accuracy: 99.32%\nEpoch 269/300, Loss: 0.0735, Accuracy: 99.63%\nEpoch 270/300, Loss: 0.0739, Accuracy: 99.47%\nEpoch 271/300, Loss: 0.0391, Accuracy: 99.67%\nEpoch 272/300, Loss: 0.0993, Accuracy: 99.35%\nEpoch 273/300, Loss: 0.1453, Accuracy: 99.29%\nEpoch 274/300, Loss: 0.1317, Accuracy: 99.27%\nEpoch 275/300, Loss: 0.1640, Accuracy: 99.10%\nEpoch 276/300, Loss: 0.2698, Accuracy: 98.80%\nEpoch 277/300, Loss: 0.1231, Accuracy: 99.27%\nEpoch 278/300, Loss: 0.0888, Accuracy: 99.39%\nEpoch 279/300, Loss: 0.1176, Accuracy: 99.28%\nEpoch 280/300, Loss: 0.1104, Accuracy: 99.37%\nEpoch 281/300, Loss: 0.2167, Accuracy: 99.08%\nEpoch 282/300, Loss: 0.0639, Accuracy: 99.53%\nEpoch 283/300, Loss: 0.1461, Accuracy: 99.26%\nEpoch 284/300, Loss: 0.1379, Accuracy: 99.30%\nEpoch 285/300, Loss: 0.2077, Accuracy: 99.09%\nEpoch 286/300, Loss: 0.1076, Accuracy: 99.42%\nEpoch 287/300, Loss: 0.0466, Accuracy: 99.74%\nEpoch 288/300, Loss: 0.0298, Accuracy: 99.75%\nEpoch 289/300, Loss: 0.1882, Accuracy: 99.15%\nEpoch 290/300, Loss: 0.1345, Accuracy: 99.27%\nEpoch 291/300, Loss: 0.0539, Accuracy: 99.64%\nEpoch 292/300, Loss: 0.1168, Accuracy: 99.44%\nEpoch 293/300, Loss: 0.0809, Accuracy: 99.53%\nEpoch 294/300, Loss: 0.2453, Accuracy: 98.92%\nEpoch 295/300, Loss: 0.1526, Accuracy: 99.24%\nEpoch 296/300, Loss: 0.1359, Accuracy: 99.40%\nEpoch 297/300, Loss: 0.1068, Accuracy: 99.48%\nEpoch 298/300, Loss: 0.0894, Accuracy: 99.51%\nEpoch 299/300, Loss: 0.1311, Accuracy: 99.25%\nEpoch 300/300, Loss: 0.2822, Accuracy: 98.84%\nTest Loss: 7.3927, Test Accuracy: 88.21%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}